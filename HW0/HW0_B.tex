\documentclass{article}

\usepackage{amsmath,amsfonts,amsthm,amssymb,amsopn,bm}
\usepackage[margin=.9in]{geometry}
\usepackage{graphicx}
\usepackage{url}
\usepackage[usenames,dvipsnames]{color}
\usepackage{fancyhdr}
\usepackage{multirow}
\usepackage{minted}
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\field{R}} % real domain
% \newcommand{\C}{\field{C}} % complex domain
\newcommand{\F}{\field{F}} % functional domain

\newcommand{\T}{^{\textrm T}} % transpose

\def\diag{\text{diag}}

%% operator in linear algebra, functional analysis
\newcommand{\inner}[2]{#1\cdot #2}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\twonorm}[1]{\|#1\|_2^2}
% operator in functios, maps such as M: domain1 --> domain 2
\newcommand{\Map}[1]{\mathcal{#1}}
\renewcommand{\theenumi}{\alph{enumi}} 

\newcommand{\Perp}{\perp \! \! \! \perp}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\vct}[1]{\boldsymbol{#1}} % vector
\newcommand{\mat}[1]{\boldsymbol{#1}} % matrix
\newcommand{\cst}[1]{\mathsf{#1}} % constant
\newcommand{\ProbOpr}[1]{\mathbb{#1}}
\newcommand{\points}[1]{\small\textcolor{magenta}{\emph{[#1 points]}} \normalsize}
\date{{}}

\setlength\parindent{0px}

\begin{document}
\title{Homework \#0 - B}
\author{\normalsize{Spring 2020, CSE 446/546: Machine Learning}\\
\normalsize{Richy Yun} \\
\normalsize{Due: 4/8/19  11:59 PM}}
\maketitle


\section*{Probability and Statistics}
B.1  \points{1} To calculate the expected value, we want to find the CDF then calculate the PDF:
\begin{center}
	$CDF_Y=p[\text{max}\{X_1,\dots,X_n\}\leq x]$
\end{center}
Since the maximum means everything is less than or equal to, and all $X$ are independent and identically distribtued, we can rewrite the above as:
\begin{center}
		$\begin{aligned}
	CDF_Y&=p[X_1,\dots,X_n \leq x]\\
	&=p[X_1\leq x]p[X_2\leq x]\dots p[X_n\leq x]\\
	&=\prod_{i=1}^{n}p[X_i\leq x]\\
	&=(p[X_i\leq x])^n
	\end{aligned}$
\end{center}
The PDF is then the derivative of the CDF:
\begin{center}
	$PDF_Y=\dfrac{d}{dn}(p[X_i\leq x])^n=n(p[X_i\leq x])^{n-1}$
\end{center}

$p[X_i\leq x]$ is the CDF of $X_i$. Since they are a uniform distribution from 0 to 1:
\begin{center}
	$\begin{aligned} PDF_{X_i}=\dfrac{1}{b-a}=\dfrac{1}{1-0}=1\\
	CDF_{X_i}=\int_{0}^{x}1dt=x \end{aligned}$
\end{center}
Now we can calculate the expected value using the definition, from 0 to 1 as that is the limit of the distribution:
\begin{center}
	$\begin{aligned} 
	\E[Y]=\int_{0}^{1}x\times PDF_Ydx=\int_{0}^{1}x\times n(x)^{n-1}dx&=\int_{0}^{1}n(x)^ndx\\
	&=n\dfrac{x^{n+1}}{n+1} \qquad \text{for } x=0 \text{ to } 1\\
	&=\dfrac{n}{n+1}
	\end{aligned}$
\end{center}

\section*{Linear Algebra and Vector Calculus}
B.2 \points{1} The definition of matrix multiplication provides:
\begin{center}
	$\begin{aligned}AB_{ij}=\sum_{k=1}^{n}A_{ik}B_{kj}\end{aligned}$
\end{center}
Therefore, for the diagonals we have:
\begin{center}
	$\begin{aligned}AB_{ii}=\sum_{k=1}^{n}A_{ik}B_{ki}\\
	BA_{ii}=\sum_{k=1}^{n}B_{ik}A_{ki}	\end{aligned}$
\end{center}
We can rewrite the traces as:
\begin{center}
	$\begin{aligned}\text{Tr}(AB)=\sum_{i}ABii=\sum_{i=1}^{m}\sum_{k=1}^{n}A_{ik}B_{ki}\\
	\text{Tr}(BA)=\sum_{i}BAii=\sum_{i=1}^{n}\sum_{k=1}^{m}B_{ik}A_{ki}	\end{aligned}$
\end{center}
Now, simply by rearranging using the commutative property:
\begin{center}
	$\begin{aligned}\text{Tr}(AB)=\sum_{i=1}^{m}\sum_{k=1}^{n}A_{ik}B_{ki}=\sum_{i=1}^{n}\sum_{k=1}^{m}B_{ik}A_{ki}=\text{Tr}(BA)=	\end{aligned}$
\end{center}

B.3 \points{1} 
    \begin{enumerate}
        \item In the case $v_i$ has a size of $n\times 1$ the result of the product is a matrix of size $n\times n$. However, as the results of each row element is dependent on the first matrix, the product has columns that are linearly dependent with a rank of 1. Summing $n$ matrices of rank 1 results in a matrix with rank $n$ when $n<d$ and rank $d$ otherwise. 
        \item The minimum possible rank is 1 as each $v_i$ could be a linear combination of one another. The maximum possible is $n$ as long as $n< d$ and $d$ otherwise, as each vector could be linearly independent of one another, but rank cannot be larger than the smallest dimension. 
        \item First we need to determine the minimum and maximum possible rank of matrix $A$. The minimum rank is 1 as each column could be linear combinations of one column, and the maximum rank is $d$ since $D>d$. Thus, the product $Av_i$ also has a minimum and maximum rank of 1 and $d$ respectively. Although the result of $(Av_i)(Av_i^T)$ is a matrix of size $D\times D$ since $Av_i$ has maximum rank $d$ the resulting product has maximum rank $d$ similar to part (a). However, summing the matrices results in the sum of the ranks, so we end up with a matrix with a minimum rank $n$ assuming $n<D$ and a maximum rank $D$.   
        \item The rank of $AV$ depends on $A$ as mentioned in part (a). Therefore, regardless of the rank of $V$ the minimum and maximum rank of $AV$ is 1 and $d$ respectively. 
    \end{enumerate}



\end{document}
